{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPr1DhfSzBkBwYiX+LY/F7Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- read from a local json file for annotation, transform to index\n","- read from url for pictures, get features with encoder model provided by the author"],"metadata":{"id":"9la6MP5i2VXp"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZL_ngL97WtqU","executionInfo":{"status":"ok","timestamp":1682951485635,"user_tz":-120,"elapsed":26627,"user":{"displayName":"Alex Meng","userId":"00783091975386351721"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1434011d-6a4e-4e8c-e095-d25bf57125f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","# !pwd  # /content  # import vocab\n","!cp /content/gdrive/MyDrive/PPlusPlus/utils/build_vocab.py /content/build_vocab.py\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.autograd import Variable\n","import torch\n","from torchvision import transforms\n","import json\n","from PIL import Image\n","import requests\n","from build_vocab import Vocabulary\n","import pickle\n","import time\n","import nltk\n","from nltk.collections import Counter\n","nltk.download('punkt')"]},{"cell_type":"code","source":["embed_size=256\n","hidden_size=512\n","num_layers=1\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class EncoderCNN(nn.Module):\n","  def __init__(self, embed_size):\n","    \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n","    super(EncoderCNN, self).__init__()\n","    # BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    resnet = models.resnet152(weights='DEFAULT')\n","    modules = list(resnet.children())[:-1]      # delete the last fc layer.\n","    self.resnet = nn.Sequential(*modules)\n","    # Linear(in_features=2048, out_features=256, bias=True)\n","    self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n","    # BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n","    self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n","    self.init_weights()\n","      \n","  def init_weights(self):\n","    \"\"\"Initialize the weights.\"\"\"\n","    self.linear.weight.data.normal_(0.0, 0.02)\n","    self.linear.bias.data.fill_(0)\n","      \n","  def forward(self, images):  # return from load_image() in utils/sample.py\n","    \"\"\"Extract the image feature vectors.\"\"\"\n","    features = self.resnet(images)  # {Tensor: (1, 2048, 1, 1)}\n","    features = Variable(features.data)  # {Tensor: (1, 2048, 1, 1)}\n","    features = features.view(features.size(0), -1)  # {Tensor: (1, 2048)}\n","    features = self.bn(self.linear(features))  # {Tensor: (1, 256)}\n","    return features\n","    "],"metadata":{"id":"gx9C_RPgfcMf","executionInfo":{"status":"ok","timestamp":1682951633109,"user_tz":-120,"elapsed":232,"user":{"displayName":"Alex Meng","userId":"00783091975386351721"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["*yield* training pairs"],"metadata":{"id":"mdAVJ-TQijRQ"}},{"cell_type":"code","source":["def getTrainingPair():\n","  \"\"\"\n","  get training item in desired input format (vectors of indices)\n","  :return:\n","  features: vector after pre-trained CNN\n","  captions: to be used in embeddings = self.embed(captions)\n","  \"\"\"\n","\n","  cnn = EncoderCNN(embed_size)\n","  cnn.eval()\n","  # self.encoder.load_state_dict(torch.load(self.encoder_path, map_location={'cuda:0': 'cpu'}))\n","  cnn.load_state_dict(torch.load('/content/gdrive/MyDrive/PPlusPlus/models/vg-encoder-5-3000.pkl', \n","                                 map_location={'cuda:0': 'cpu'}))\n","  if torch.cuda.is_available():\n","    cnn.cuda()\n","  transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406),\n","                        (0.229, 0.224, 0.225))])\n","  json_data = json.loads(open('/content/gdrive/MyDrive/PPlusPlus/vg_data/region_descriptions.json', 'r').read())\n","  with open(\"/content/gdrive/MyDrive/PPlusPlus/vg_data/vocab_small.pkl\", 'rb') as f:\n","    vocab = pickle.load(f)  # a Vocabulary() from utils/build_vocab.py\n","\n","  counter = 0\n","  for each_dict in json_data:\n","    #############\n","    if counter > 41005:\n","      break\n","    #############\n","    # https://cs.stanford.edu/people/rak248/VG_100K_2/1.jpg\n","    file_url = \"https://cs.stanford.edu/people/rak248/VG_100K_2/\" + str(each_dict[\"id\"]) + \".jpg\"\n","    response = requests.get(file_url, stream=True)\n","    # print(response)  ####################\n","    if response.status_code == 200:  # only if url is accessable\n","      im = Image.open(requests.get(file_url, stream=True).raw)\n","      # each im contains a list of regions\n","      reg_list = each_dict['regions']\n","      for each_region in reg_list:\n","        ################\n","        counter += 1\n","        if counter % 10000 == 0:\n","          print(counter)\n","        if counter < 41000:\n","          continue\n","        if counter > 41005:\n","          break\n","        ################\n","        # {\"region_id\": int, \"width\": int, \"height\": int, \"image_id\": int, \"phrase\": str, \"y\": int, \"x\": int}\n","        left = each_region[\"x\"]  # x\n","        top = each_region[\"y\"]  # y\n","        right = each_region[\"x\"] + each_region[\"width\"]  # x+width\n","        bottom = each_region[\"y\"] + each_region[\"height\"]  # y+height\n","        im1 = im.crop((left, top, right, bottom))\n","        # im1.show()  ###############\n","        # resize\n","        im1 = im1.resize([224, 224])\n","        im = im.convert('RGB')\n","        # transform (1, 3, 224, 224)\n","        im1 = transform(im1).unsqueeze(0)\n","        # feed into cnn\n","        # self.encoder(to_var(load_image(url, self.transform)))\n","        # to_var() from utils/sample.py\n","        if torch.cuda.is_available():\n","          im1 = im1.cuda()  # {Tensor: (1, 3, 224, 224)}\n","        features = cnn(Variable(im1))  # cnn.forward(Variable(im1))\n","\n","        # read captions, get index\n","        caption = each_region['phrase']\n","        tokens = nltk.tokenize.word_tokenize(caption.lower())\n","        # print(tokens)  ##################\n","        captions = [vocab.word2idx[\"<start>\"]]\n","        # + [vocab.word2idx[word] for word in tokens]]\n","        len_token = 0\n","        for word in tokens:\n","          if word in vocab.word2idx:\n","            captions.append(vocab.word2idx[word])\n","          else:\n","            captions.append(vocab.word2idx[\"<unk>\"])\n","          len_token += 1  # 1 - 10\n","          if len_token > 9: break\n","        captions.append(vocab.word2idx[\"<end>\"])\n","        # padding\n","        while len_token < 10:\n","          captions.append(vocab.word2idx[\"<pad>\"])\n","          len_token += 1\n","        captions = torch.tensor(captions)\n","        yield features, captions\n","\n","# for feature, captions in getTrainingPair():\n","#   print(feature.shape)\n","#   print(feature, '\\n', captions)\n","#   break"],"metadata":{"id":"ZMRu6y1kdrMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for feature, captions in getTrainingPair():\n","  print(feature.shape, '\\n', captions)\n","  # break"],"metadata":{"id":"O_b2jKpk51o_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test/eval data 4k with features, caps in tensor, caps in text, file_url\n","def getTestData():\n","  \"\"\"\n","  get test data in desired input format (vectors of indices)\n","  :return: in 4k pairs\n","  features: vector after pre-trained CNN\n","  captions: to be used in embeddings = self.embed(captions)\n","  phrase: captions as strings\n","  file_url: link to the file\n","  \"\"\"\n","  cnn = EncoderCNN(embed_size)\n","  cnn.eval()\n","  # self.encoder.load_state_dict(torch.load(self.encoder_path, map_location={'cuda:0': 'cpu'}))\n","  cnn.load_state_dict(torch.load('/content/gdrive/MyDrive/PPlusPlus/models/vg-encoder-5-3000.pkl', \n","                                 map_location={'cuda:0': 'cpu'}))\n","  if torch.cuda.is_available():\n","    cnn.cuda()\n","  transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406),\n","                        (0.229, 0.224, 0.225))])\n","  json_data = json.loads(open('/content/gdrive/MyDrive/PPlusPlus/vg_data/region_descriptions.json', 'r').read())\n","  with open(\"/content/gdrive/MyDrive/PPlusPlus/vg_data/vocab_small.pkl\", 'rb') as f:\n","    vocab = pickle.load(f)  # a Vocabulary() from utils/build_vocab.py\n","\n","  counter = 0\n","  for each_dict in json_data:\n","    # https://cs.stanford.edu/people/rak248/VG_100K_2/1.jpg\n","    file_url = \"https://cs.stanford.edu/people/rak248/VG_100K_2/\" + str(each_dict[\"id\"]) + \".jpg\"\n","    response = requests.get(file_url, stream=True)\n","    # print(response)  ####################\n","    if response.status_code == 200:  # only if url is accessable\n","      im = Image.open(requests.get(file_url, stream=True).raw)\n","      if im.mode != 'RGB': continue  ######################################\n","      # each im contains a list of regions\n","      reg_list = each_dict['regions']\n","      for each_region in reg_list:\n","        ################\n","        counter += 1\n","        if counter < 40001:\n","          if counter % 10000 == 0:\n","            print('skipping ', str(counter))\n","          continue\n","        ################\n","        # {\"region_id\": int, \"width\": int, \"height\": int, \"image_id\": int, \"phrase\": str, \"y\": int, \"x\": int}\n","        left = each_region[\"x\"]  # x\n","        top = each_region[\"y\"]  # y\n","        right = each_region[\"x\"] + each_region[\"width\"]  # x+width\n","        bottom = each_region[\"y\"] + each_region[\"height\"]  # y+height\n","        im1 = im.crop((left, top, right, bottom))\n","        # im1.show()  ###############\n","        # resize\n","        im1 = im1.resize([224, 224])\n","        # transform (1, 3, 224, 224)\n","        im1 = transform(im1).unsqueeze(0)\n","        # feed into cnn\n","        # self.encoder(to_var(load_image(url, self.transform)))\n","        # to_var() from utils/sample.py\n","        if torch.cuda.is_available():\n","          im1 = im1.cuda()  # {Tensor: (1, 3, 224, 224)}\n","        features = cnn(Variable(im1))  # cnn.forward(Variable(im1))\n","\n","        # read captions, get index\n","        phrase = each_region['phrase']\n","        tokens = nltk.tokenize.word_tokenize(phrase.lower())\n","        # print(tokens)  ##################\n","        captions = [vocab.word2idx[\"<start>\"]]\n","        # + [vocab.word2idx[word] for word in tokens]]\n","        len_token = 0\n","        for word in tokens:\n","          if word in vocab.word2idx:\n","            captions.append(vocab.word2idx[word])\n","          else:\n","            captions.append(vocab.word2idx[\"<unk>\"])\n","          len_token += 1  # 1 - 10\n","          if len_token > 9: break\n","        captions.append(vocab.word2idx[\"<end>\"])\n","        # padding\n","        while len_token < 10:\n","          captions.append(vocab.word2idx[\"<pad>\"])\n","          len_token += 1\n","        captions = torch.tensor(captions)\n","        yield features, captions, phrase, file_url\n"],"metadata":{"id":"zcU993ZAFHXG","executionInfo":{"status":"ok","timestamp":1682952286583,"user_tz":-120,"elapsed":197,"user":{"displayName":"Alex Meng","userId":"00783091975386351721"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# yield 4k test/eval data \n","batch_temp = []  # batch container\n","counter = 0\n","start = time.time()\n","file_index = 0\n","for item in getTestData():\n","  batch_temp.append(item)\n","  counter += 1\n","  #########\n","  # if counter > 3: break\n","  #########\n","  if counter % 4000 == 0:  # save every 40k in one pkl\n","    filename = 'vg_test_' + str(file_index) + \".pkl\"\n","    file_index += 1\n","    with open(filename, 'wb') as fp:\n","      torch.save(batch_temp, fp)\n","    batch_temp = []  # reset the container\n","    print(str(counter), \"\\t\", str((time.time() - start)/60), \"\\t\", filename)\n","  elif counter % 2000 == 0:  # time tracking\n","    print(counter, \"\\t\", str((time.time() - start)/60))\n","\n","if len(batch_temp) != 0:  # save the last bits that is fewer than 40k\n","  print(str(len(batch_temp)) + \" pairs remaining\")\n","  filename = 'vg_test_' + str(file_index) + \".pkl\"\n","  with open(filename, 'wb') as fp:\n","    torch.save(batch_temp, fp)\n","\n","print(str(counter), \" pairs in total\")\n","print('finished in ', str((time.time() - start)/60))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":811},"id":"acjRwzdwJQGp","executionInfo":{"status":"error","timestamp":1682954244847,"user_tz":-120,"elapsed":1943038,"user":{"displayName":"Alex Meng","userId":"00783091975386351721"}},"outputId":"3e8e8b3b-4cde-4cc2-cd80-d7b664e0318c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["skipping  10000\n","skipping  20000\n","skipping  30000\n","skipping  40000\n","2000 \t 8.910920055707296\n","4000 \t 10.13270030816396 \t vg_test_0.pkl\n","6000 \t 11.314764889081319\n","8000 \t 12.504759899775188 \t vg_test_1.pkl\n","10000 \t 13.700434291362763\n","12000 \t 14.887558635075887 \t vg_test_2.pkl\n","14000 \t 16.077542901039124\n","16000 \t 17.270712522665658 \t vg_test_3.pkl\n","18000 \t 18.440325804551442\n","20000 \t 19.63114091157913 \t vg_test_4.pkl\n","22000 \t 20.797709925969443\n","24000 \t 21.97004839579264 \t vg_test_5.pkl\n","26000 \t 23.298837800820667\n","28000 \t 24.51781755288442 \t vg_test_6.pkl\n","30000 \t 25.685112857818602\n","32000 \t 26.87875454823176 \t vg_test_7.pkl\n","34000 \t 28.046668362617492\n","36000 \t 29.20089542865753 \t vg_test_8.pkl\n","38000 \t 30.375420423348746\n","40000 \t 31.5208336631457 \t vg_test_9.pkl\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-962d3f560435>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetTestData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mbatch_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-6bf8a2c3b287>\u001b[0m in \u001b[0;36mgetTestData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           \u001b[0mim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# {Tensor: (1, 3, 224, 224)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# cnn.forward(Variable(im1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# read captions, get index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-67222c796192>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# return from load_image() in utils/sample.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m\"\"\"Extract the image feature vectors.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# {Tensor: (1, 2048, 1, 1)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# {Tensor: (1, 2048, 1, 1)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# {Tensor: (1, 2048)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["|counter|time|file_name|\n","| ----- | -- | ------- |\n"],"metadata":{"id":"pDcA_CsapYpR"}},{"cell_type":"code","source":["with open('/content/vg_test_6.pkl', 'rb') as f:\n","  data = torch.load(f, map_location=device)\n","\n","counter = 0\n","for features, captions, phrase, file_url in data:\n","  print(features.shape)\n","  print(captions)\n","  print(phrase)\n","  print(file_url)\n","  counter += 1\n","  if counter > 3: break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fztBvFLhKWih","executionInfo":{"status":"ok","timestamp":1682954301264,"user_tz":-120,"elapsed":1100,"user":{"displayName":"Alex Meng","userId":"00783091975386351721"}},"outputId":"85e32334-6c47-4cb9-d0fa-c2bf9bd356c9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 256])\n","tensor([  1,  39, 643,  27,   4, 384,   2,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","white plate on the table \n","https://cs.stanford.edu/people/rak248/VG_100K_2/1342.jpg\n","torch.Size([1, 256])\n","tensor([   1,  221,  100, 1377,    8,   72,    2,    0,    0,    0,    0,    0],\n","       device='cuda:0')\n","Glass with ice in it \n","https://cs.stanford.edu/people/rak248/VG_100K_2/1342.jpg\n","torch.Size([1, 256])\n","tensor([   1,  365,   14,   25, 2852,    2,    0,    0,    0,    0,    0,    0],\n","       device='cuda:0')\n","woman wearing a apron \n","https://cs.stanford.edu/people/rak248/VG_100K_2/1342.jpg\n","torch.Size([1, 256])\n","tensor([   1, 1932,   29,  892,   27,    4,  384,    2,    0,    0,    0,    0],\n","       device='cuda:0')\n","bouquet of flowers on the table \n","https://cs.stanford.edu/people/rak248/VG_100K_2/1342.jpg\n"]}]},{"cell_type":"markdown","source":["pickle feature and caption pairs in 40ks for training"],"metadata":{"id":"SYVfIorRjgef"}},{"cell_type":"code","source":["# yield 40k pairs for training\n","batch_temp = []  # batch container\n","counter = 0\n","last_time = time.time()\n","file_index = 0\n","for feature, captions in getTrainingPair():\n","  batch_temp.append((feature, captions))\n","  counter += 1\n","  if counter % 10000 == 0:  # save every 40k in one pkl\n","    filename = 'vg_feat_cap_' + str(file_index) + \".pkl\"\n","    file_index += 1\n","    with open(filename, 'wb') as fp:\n","      torch.save(batch_temp, fp)\n","    batch_temp = []  # reset the container\n","    print(str(counter), \"\\t\", str((time.time() - last_time)/60), \"\\t\", filename)\n","    last_time = time.time()\n","  elif counter % 10000 == 0:  # time tracking\n","    print(counter, \"\\t\", str((time.time() - last_time)/60))\n","    last_time = time.time()\n","\n","if len(batch_temp) != 0:  # save the last bits that is fewer than 40k\n","  print(str(len(batch_temp)) + \" pairs remaining\")\n","  filename = 'vg_feat_cap_' + str(file_index) + \".pkl\"\n","  with open(filename, 'wb') as fp:\n","    torch.save(batch_temp, fp)\n","\n","print(str(counter), \" pairs in total\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475},"id":"OsW6CwI8jf57","executionInfo":{"status":"error","timestamp":1682372123155,"user_tz":-120,"elapsed":1300157,"user":{"displayName":"Alex Meng","userId":"00783091975386351721"}},"outputId":"7206d566-5c93-47a0-c46c-428289f3af49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10000 \t 5.704886988798777\n","20000 \t 5.222293734550476\n","30000 \t 5.351386781533559\n","40000 \t 5.18360896507899 \t vg_feat_cap_0.pkl\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e6ad81a86848>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlast_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetTrainingPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mbatch_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-40a698a2808c>\u001b[0m in \u001b[0;36mgetTrainingPair\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# transform (1, 3, 224, 224)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# feed into cnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]"]}]},{"cell_type":"markdown","source":["| ks | 10 | 20 | 30 | 40 |\n","| -- | -- | -- | -- | -- |\n","|mins|5.70|5.22|5.35|5.18|\n"],"metadata":{"id":"0NHHAQlFH7Hn"}},{"cell_type":"markdown","source":["7 mins/10k, 30 mins/dump"],"metadata":{"id":"6_8gQXcIRu2k"}},{"cell_type":"code","source":["# #To load from pickle file\n","# data = []\n","# with open(filename, 'rb') as fr:\n","#   try:\n","#     while True:\n","#       data.append(pickle.load(fr))\n","#   except EOFError:\n","#       pass\n","\n","# print(data)\n","\n","with open('/content/vg_feat_cap_0.pkl', 'rb') as f:\n","  data = torch.load(f, map_location=device)\n","for i, j in data:\n","  print(i.shape, j.shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dF1RCJKIF8Wr","executionInfo":{"status":"ok","timestamp":1682370524477,"user_tz":-120,"elapsed":234,"user":{"displayName":"Alex Meng","userId":"00783091975386351721"}},"outputId":"f21590c4-0570-41df-cc72-690ce70a0a5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 256]) torch.Size([12])\n","torch.Size([1, 256]) torch.Size([12])\n","torch.Size([1, 256]) torch.Size([12])\n","torch.Size([1, 256]) torch.Size([12])\n"]}]}]}